{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016373ba",
   "metadata": {},
   "source": [
    "# 02 — Train & Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c943ff-722d-4ad2-a6bc-532fa39fd42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 235, 234)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import shap  # KernelExplainer later (robust for small tabular)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# paths\n",
    "ART = Path(\"../artifacts\")\n",
    "ASSETS = Path(\"../assets\")\n",
    "MODELS = Path(\"../models\")\n",
    "ASSETS.joinpath(\"gradcam\").mkdir(parents=True, exist_ok=True)\n",
    "ASSETS.joinpath(\"shap\").mkdir(parents=True, exist_ok=True)\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load splits from Notebook 1\n",
    "train_df = pd.read_parquet(ART / \"train.parquet\")\n",
    "val_df   = pd.read_parquet(ART / \"val.parquet\")\n",
    "test_df  = pd.read_parquet(ART / \"test.parquet\")\n",
    "len(train_df), len(val_df), len(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c677dbef-c2e8-42fa-9732-a77f5f9f330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "class OvarianDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _resolve_path(self, p):\n",
    "        p = Path(p)\n",
    "        # If relative like \"data/images/..\", prepend project root (\"..\") when running from notebooks/\n",
    "        if not p.is_absolute() and not str(p).startswith(\"..\"):\n",
    "            p = Path(\"..\") / p\n",
    "        return p\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self._resolve_path(row[\"image_path\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        tabular = torch.tensor([row[\"age\"], row[\"ca125\"], row[\"brca\"]], dtype=torch.float32)\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "        return image, tabular, label\n",
    "\n",
    "BATCH = 32\n",
    "train_loader = DataLoader(OvarianDataset(train_df, transform), batch_size=BATCH, shuffle=True, num_workers=2 if torch.cuda.is_available() else 0)\n",
    "val_loader   = DataLoader(OvarianDataset(val_df, transform), batch_size=BATCH, shuffle=False, num_workers=2 if torch.cuda.is_available() else 0)\n",
    "test_loader  = DataLoader(OvarianDataset(test_df, transform), batch_size=BATCH, shuffle=False, num_workers=2 if torch.cuda.is_available() else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2c1495-51a1-489e-be4b-db6a238ce6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularMLP(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def get_img_encoder():\n",
    "    base = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)  # patched\n",
    "    for p in base.parameters():\n",
    "        p.requires_grad = False\n",
    "    in_feat = base.fc.in_features\n",
    "    base.fc = nn.Identity()\n",
    "    return base, in_feat\n",
    "\n",
    "class ImageOnly(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone, feat_dim = get_img_encoder()\n",
    "        self.fc = nn.Linear(feat_dim, 1)\n",
    "    def forward(self, img, tab=None):\n",
    "        feats = self.backbone(img)\n",
    "        return self.fc(feats)\n",
    "\n",
    "class FusedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone, feat_dim = get_img_encoder()\n",
    "        self.tab_mlp = TabularMLP()\n",
    "        self.fc = nn.Linear(feat_dim + 1, 1)  # tab_mlp returns 1‑dim logit\n",
    "    def forward(self, img, tab):\n",
    "        img_feat = self.backbone(img)\n",
    "        tab_feat = self.tab_mlp(tab)\n",
    "        x = torch.cat([img_feat, tab_feat], dim=1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c084d5fa-a1ca-4a97-a62f-084e0da978c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_any(model, imgs, tabs):\n",
    "    try:\n",
    "        return model(imgs, tabs)\n",
    "    except TypeError:\n",
    "        return model(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95f2b4ef-8819-4e7b-8767-d205881c4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=8, lr=1e-3, name=\"model\"):\n",
    "    model = model.to(device)\n",
    "    crit = nn.BCEWithLogitsLoss()\n",
    "    opt  = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_auc, best_state = 0.0, None\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        for imgs, tabs, labels in train_loader:\n",
    "            imgs, tabs = imgs.to(device), tabs.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            opt.zero_grad()\n",
    "            logits = forward_any(model, imgs, tabs)\n",
    "            loss = crit(logits, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # validation (AUC for early stopping)\n",
    "        model.eval()\n",
    "        y_true, y_prob = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, tabs, labels in val_loader:\n",
    "                imgs, tabs = imgs.to(device), tabs.to(device)\n",
    "                logits = forward_any(model, imgs, tabs)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "                y_prob.extend(probs)\n",
    "                y_true.extend(labels.numpy().ravel())\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}  |  Val AUC: {auc:.4f}\")\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    torch.save(model.state_dict(), MODELS / f\"{name}.pth\")\n",
    "    print(f\"Saved best '{name}' (AUC={best_auc:.4f}) -> {MODELS/f'{name}.pth'}\")\n",
    "    return model\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, tabs, labels in loader:\n",
    "            imgs, tabs = imgs.to(device), tabs.to(device)\n",
    "            logits = forward_any(model, imgs, tabs)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n",
    "            y_prob.extend(probs)\n",
    "            y_true.extend(labels.numpy().ravel())\n",
    "    y_pred = (np.array(y_prob) > 0.5).astype(int)\n",
    "    return {\n",
    "        \"accuracy\":  float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred)),\n",
    "        \"recall\":    float(recall_score(y_true, y_pred)),\n",
    "        \"f1\":        float(f1_score(y_true, y_pred)),\n",
    "        \"roc_auc\":   float(roc_auc_score(y_true, y_prob)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ec13be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8  |  Val AUC: 0.4994\n",
      "Epoch 2/8  |  Val AUC: 0.4670\n",
      "Epoch 3/8  |  Val AUC: 0.4637\n",
      "Epoch 4/8  |  Val AUC: 0.5033\n",
      "Epoch 5/8  |  Val AUC: 0.5044\n",
      "Epoch 6/8  |  Val AUC: 0.5281\n",
      "Epoch 7/8  |  Val AUC: 0.5743\n",
      "Epoch 8/8  |  Val AUC: 0.5997\n",
      "Saved best 'image_only' (AUC=0.5997) -> ..\\models\\image_only.pth\n",
      "Epoch 1/8  |  Val AUC: 0.2318\n",
      "Epoch 2/8  |  Val AUC: 0.4020\n",
      "Epoch 3/8  |  Val AUC: 0.6057\n",
      "Epoch 4/8  |  Val AUC: 0.8431\n",
      "Epoch 5/8  |  Val AUC: 0.9681\n",
      "Epoch 6/8  |  Val AUC: 0.9857\n",
      "Epoch 7/8  |  Val AUC: 0.9950\n",
      "Epoch 8/8  |  Val AUC: 0.9994\n",
      "Saved best 'tabular_only' (AUC=0.9994) -> ..\\models\\tabular_only.pth\n",
      "Epoch 1/8  |  Val AUC: 0.4642\n",
      "Epoch 2/8  |  Val AUC: 0.4466\n",
      "Epoch 3/8  |  Val AUC: 0.4719\n",
      "Epoch 4/8  |  Val AUC: 0.5165\n",
      "Epoch 5/8  |  Val AUC: 0.6085\n",
      "Epoch 6/8  |  Val AUC: 0.7197\n",
      "Epoch 7/8  |  Val AUC: 0.8458\n",
      "Epoch 8/8  |  Val AUC: 0.9202\n",
      "Saved best 'fused' (AUC=0.9202) -> ..\\models\\fused.pth\n"
     ]
    }
   ],
   "source": [
    "img_model   = train_model(ImageOnly(),   train_loader, val_loader, num_epochs=8, name=\"image_only\")\n",
    "tab_model   = train_model(TabularMLP(),  train_loader, val_loader, num_epochs=8, name=\"tabular_only\")\n",
    "fused_model = train_model(FusedModel(),  train_loader, val_loader, num_epochs=8, name=\"fused\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53aba4eb-6998-40fd-8562-45ed6b6d2187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Hitesh\\Univ\\LunarAI-Assessment\\SeleneX_Assignment5_Starter\\lunartech\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\Hitesh\\Univ\\LunarAI-Assessment\\SeleneX_Assignment5_Starter\\lunartech\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"image_only\": {\n",
      "    \"accuracy\": 0.9700854700854701,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"roc_auc\": 0.8023914411579609\n",
      "  },\n",
      "  \"tabular_only\": {\n",
      "    \"accuracy\": 0.9700854700854701,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"roc_auc\": 1.0\n",
      "  },\n",
      "  \"fused\": {\n",
      "    \"accuracy\": 0.9700854700854701,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"roc_auc\": 0.9559471365638766\n",
      "  }\n",
      "}\n",
      "Wrote metrics -> ..\\artifacts\\metrics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Hitesh\\Univ\\LunarAI-Assessment\\SeleneX_Assignment5_Starter\\lunartech\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"image_only\":  evaluate(img_model,   test_loader),\n",
    "    \"tabular_only\":evaluate(tab_model,   test_loader),\n",
    "    \"fused\":       evaluate(fused_model, test_loader),\n",
    "}\n",
    "print(json.dumps(results, indent=2))\n",
    "\n",
    "with open(ART/\"metrics.json\",\"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Wrote metrics ->\", ART/\"metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6e1c68d-e21d-4513-ba3a-bf2d0285026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\assets\\gradcam\\1378_gradcam.png\n",
      "Saved: ..\\assets\\gradcam\\930_gradcam.png\n",
      "Saved: ..\\assets\\gradcam\\1432_gradcam.png\n",
      "Saved: ..\\assets\\gradcam\\1430_gradcam.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def gradcam_single(model, img_tensor, target_layer):\n",
    "    acts, grads = {}, {}\n",
    "\n",
    "    def fwd(m, i, o):\n",
    "        acts[\"v\"] = o\n",
    "        # attach a tensor-level hook to capture dL/d(acts)\n",
    "        o.register_hook(lambda g: grads.setdefault(\"v\", g))\n",
    "\n",
    "    h = target_layer.register_forward_hook(fwd)\n",
    "\n",
    "    model.eval()\n",
    "    # tab dummy for image-only\n",
    "\n",
    "    img_tensor = img_tensor.to(device).requires_grad_(True)\n",
    "    \n",
    "    out = model(img_tensor.to(device), torch.zeros((1,3), device=device))\n",
    "    logit = out.mean()\n",
    "    model.zero_grad()\n",
    "    logit.backward()\n",
    "\n",
    "    h.remove()\n",
    "\n",
    "    A = acts[\"v\"].detach()                    # [B, C, H, W]\n",
    "    G = grads[\"v\"].detach()                   # [B, C, H, W]\n",
    "    weights = G.mean(dim=(2,3), keepdim=True) # [B, C, 1, 1]\n",
    "    cam = F.relu((weights * A).sum(dim=1)).squeeze().cpu().numpy()\n",
    "    cam = (cam - cam.min()) / (cam.max() + 1e-8)\n",
    "    return cam\n",
    "\n",
    "def overlay_cam(rgb_img_224, cam):\n",
    "    # cam: HxW (e.g., 7x7); upsample to image size\n",
    "    img_np = np.asarray(rgb_img_224)              # (224,224,3)\n",
    "    H, W = img_np.shape[:2]\n",
    "    cam_up = cv2.resize(cam, (W, H), interpolation=cv2.INTER_CUBIC)\n",
    "    heatmap = cv2.applyColorMap((cam_up*255).astype(np.uint8), cv2.COLORMAP_JET)[:, :, ::-1] / 255.0\n",
    "    overlay = 0.4*heatmap + 0.6*(img_np/255.0)\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    return (overlay*255).astype(np.uint8)\n",
    "\n",
    "# pick 2 benign + 2 malignant from test_df\n",
    "sel_benign = test_df[test_df[\"label\"]==0].sample(min(2, (test_df[\"label\"]==0).sum()), random_state=0)\n",
    "sel_malign = test_df[test_df[\"label\"]==1].sample(min(2, (test_df[\"label\"]==1).sum()), random_state=0)\n",
    "sel = pd.concat([sel_benign, sel_malign], ignore_index=True)\n",
    "\n",
    "target_layer = img_model.backbone.layer4[-1]\n",
    "\n",
    "for _, row in sel.iterrows():\n",
    "    p = row[\"image_path\"]\n",
    "    p = Path(p) if str(p).startswith(\"..\") or Path(p).is_absolute() else Path(\"..\")/p\n",
    "    img = Image.open(p).convert(\"RGB\").resize((224,224))\n",
    "    tens = transform(img).unsqueeze(0)\n",
    "\n",
    "    cam = gradcam_single(img_model, tens, target_layer)\n",
    "    overlay = overlay_cam(img, cam)\n",
    "    out_name = ASSETS/\"gradcam\"/f\"{Path(p).stem}_gradcam.png\"\n",
    "    Image.fromarray(overlay).save(out_name)\n",
    "    print(\"Saved:\", out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32611e06-8457-4f8f-8636-b3822dd741e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 129.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ..\\assets\\shap\\global.png\n",
      "Saved: ..\\assets\\shap\\local_0.png\n",
      "Saved: ..\\assets\\shap\\local_1.png\n"
     ]
    }
   ],
   "source": [
    "tab_model.eval()\n",
    "\n",
    "def tab_predict(x_np):\n",
    "    x = torch.tensor(x_np, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        logits = tab_model(x).numpy().ravel()\n",
    "    return 1.0 / (1.0 + np.exp(-logits))\n",
    "\n",
    "X_train = train_df[[\"age\",\"ca125\",\"brca\"]].values\n",
    "X_test  = test_df[[\"age\",\"ca125\",\"brca\"]].values\n",
    "\n",
    "bg = X_train[np.random.RandomState(0).choice(len(X_train), size=min(100, len(X_train)), replace=False)]\n",
    "explainer = shap.KernelExplainer(tab_predict, bg)\n",
    "\n",
    "X_sample = X_test[: min(200, len(X_test))]\n",
    "shap_values = explainer.shap_values(X_sample, nsamples=100)\n",
    "\n",
    "# Global\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=[\"age\",\"ca125\",\"brca\"], show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(ASSETS/\"shap/global.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved:\", ASSETS/\"shap/global.png\")\n",
    "\n",
    "# Two locals\n",
    "for i in range(min(2, len(X_sample))):\n",
    "    shap.force_plot(explainer.expected_value, shap_values[i], X_sample[i],\n",
    "                    feature_names=[\"age\",\"ca125\",\"brca\"], matplotlib=True, show=False)\n",
    "    plt.tight_layout()\n",
    "    outp = ASSETS/f\"shap/local_{i}.png\"\n",
    "    plt.savefig(outp, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1deb95d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FusedModel(\n",
       "  (backbone): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (tab_mlp): TabularMLP(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=16, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=16, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=513, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def strip_all_hooks(model: torch.nn.Module):\n",
    "    # wipe all hooks on every submodule\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, \"_forward_hooks\") and isinstance(m._forward_hooks, dict):\n",
    "            m._forward_hooks = OrderedDict()\n",
    "        if hasattr(m, \"_forward_pre_hooks\") and isinstance(m._forward_pre_hooks, dict):\n",
    "            m._forward_pre_hooks = OrderedDict()\n",
    "        if hasattr(m, \"_backward_hooks\") and isinstance(m._backward_hooks, dict):\n",
    "            m._backward_hooks = OrderedDict()\n",
    "\n",
    "# Clear hooks and re-eval\n",
    "strip_all_hooks(img_model)\n",
    "strip_all_hooks(tab_model)\n",
    "strip_all_hooks(fused_model)\n",
    "\n",
    "# (optional but clean) reload weights to guarantee a fresh graph\n",
    "img_model.load_state_dict(torch.load(MODELS/\"image_only.pth\", map_location=device))\n",
    "tab_model.load_state_dict(torch.load(MODELS/\"tabular_only.pth\", map_location=device))\n",
    "fused_model.load_state_dict(torch.load(MODELS/\"fused.pth\", map_location=device))\n",
    "\n",
    "img_model.eval(); tab_model.eval(); fused_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dd81e4e-76b6-40ce-94e4-4a954eaa3ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Hitesh\\Univ\\LunarAI-Assessment\\SeleneX_Assignment5_Starter\\lunartech\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\Hitesh\\Univ\\LunarAI-Assessment\\SeleneX_Assignment5_Starter\\lunartech\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\Hitesh\\Univ\\LunarAI-Assessment\\SeleneX_Assignment5_Starter\\lunartech\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"image_only\": {\n",
      "    \"accuracy\": 0.9700854700854701,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"roc_auc\": 0.8023914411579609\n",
      "  },\n",
      "  \"tabular_only\": {\n",
      "    \"accuracy\": 0.9700854700854701,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"roc_auc\": 1.0\n",
      "  },\n",
      "  \"fused\": {\n",
      "    \"accuracy\": 0.9700854700854701,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"roc_auc\": 0.9559471365638766\n",
      "  }\n",
      "}\n",
      "Saved metrics.json and ROC/CM plots in assets/metrics/\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def collect_preds(model, loader):\n",
    "    model.eval()\n",
    "    y_true, y_prob = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, tabs, labels in loader:\n",
    "            imgs, tabs = imgs.to(device), tabs.to(device)\n",
    "            logits = forward_any(model, imgs, tabs)\n",
    "            y_prob.extend(torch.sigmoid(logits).cpu().numpy().ravel())\n",
    "            y_true.extend(labels.numpy().ravel())\n",
    "    y_true = np.array(y_true); y_prob = np.array(y_prob)\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    return y_true, y_prob, y_pred\n",
    "\n",
    "def dump_model_results(name, model):\n",
    "    y_true, y_prob, y_pred = collect_preds(model, test_loader)\n",
    "    res = {\n",
    "        \"accuracy\":  float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred)),\n",
    "        \"recall\":    float(recall_score(y_true, y_pred)),\n",
    "        \"f1\":        float(f1_score(y_true, y_pred)),\n",
    "        \"roc_auc\":   float(roc_auc_score(y_true, y_prob)),\n",
    "    }\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={res['roc_auc']:.3f})\")\n",
    "    plt.plot([0,1],[0,1],\"--\",alpha=0.5)\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC - {name}\"); plt.legend()\n",
    "    (ASSETS/\"metrics\").mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(ASSETS/f\"metrics/roc_{name}.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(3.2,3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(f\"CM - {name}\")\n",
    "    plt.savefig(ASSETS/f\"metrics/cm_{name}.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    return res\n",
    "\n",
    "results = {\n",
    "    \"image_only\":  dump_model_results(\"image_only\", img_model),\n",
    "    \"tabular_only\":dump_model_results(\"tabular_only\", tab_model),\n",
    "    \"fused\":       dump_model_results(\"fused\", fused_model),\n",
    "}\n",
    "with open(ART/\"metrics.json\",\"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(json.dumps(results, indent=2))\n",
    "print(\"Saved metrics.json and ROC/CM plots in assets/metrics/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bfa84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lunartech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
